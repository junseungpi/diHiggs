{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c028c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import sys\n",
    "import cmath\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90764d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetTrainData(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.x = torch.from_numpy( np.load('/data/github/data/diHiggs_neutrino_train_data.npy').astype(np.float32) )\n",
    "        #                                               hh               tt             tw             tth            ttv            llbj           tatabb\n",
    "        self.y = torch.from_numpy( np.concatenate((np.ones(19000), np.zeros(9900), np.zeros(5500), np.zeros(200), np.zeros(250), np.zeros(1000), np.zeros(60))).astype(np.int) )\n",
    "        self.n_samples = 35910\n",
    "                \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    \n",
    "class JetTestData(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.x = torch.from_numpy( np.load('/data/github/data/diHiggs_neutrino_test_data.npy').astype(np.float32) )\n",
    "        #                                               hh              tt              tw            tth            ttv            llbj          tatabb\n",
    "        self.y = torch.from_numpy( np.concatenate((np.ones(5000), np.zeros(19000), np.zeros(800), np.zeros(100), np.zeros(100), np.zeros(240), np.zeros(10))).astype(np.int) )\n",
    "        self.n_samples = 25250\n",
    "                \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_data = JetTrainData()\n",
    "test_data = JetTestData()\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size = batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size = batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, int(batch_size/2), idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(images[idx,2], cmap='gray')\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, A, B, K, P, stride):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        self.pose = nn.Conv2d(in_channels=A, out_channels=B*P*P,\n",
    "                            kernel_size=K, stride=stride, bias=True)\n",
    "        self.a = nn.Conv2d(in_channels=A, out_channels=B,\n",
    "                            kernel_size=K, stride=stride, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        p = self.pose(x)\n",
    "        a = self.a(x)\n",
    "        a = self.sigmoid(a)\n",
    "        out = torch.cat([p, a], dim=1)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59828dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCaps(nn.Module):\n",
    "    def __init__(self, B, C, K, P, stride, iters,\n",
    "                 coor_add=False, w_shared=False):\n",
    "        super(ConvCaps, self).__init__()\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.K = K\n",
    "        self.P = P\n",
    "        self.psize = P*P\n",
    "        self.stride = stride\n",
    "        self.iters = iters\n",
    "        self.coor_add = coor_add\n",
    "        self.w_shared = w_shared\n",
    "        self.eps = 1e-8\n",
    "        self._lambda = 1e-03\n",
    "        self.ln_2pi = torch.cuda.FloatTensor(1).fill_(math.log(2*math.pi))\n",
    "        self.beta_u = nn.Parameter(torch.zeros(C))\n",
    "        self.beta_a = nn.Parameter(torch.zeros(C))\n",
    "        self.weights = nn.Parameter(torch.randn(1, K*K*B, C, P, P))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def m_step(self, a_in, r, v, eps, b, B, C, psize):\n",
    "        r = r * a_in\n",
    "        r = r / (r.sum(dim=2, keepdim=True) + eps)\n",
    "        r_sum = r.sum(dim=1, keepdim=True)\n",
    "        coeff = r / (r_sum + eps)\n",
    "        coeff = coeff.view(b, B, C, 1)\n",
    "\n",
    "        mu = torch.sum(coeff * v, dim=1, keepdim=True)\n",
    "        sigma_sq = torch.sum(coeff * (v - mu)**2, dim=1, keepdim=True) + eps\n",
    "\n",
    "        r_sum = r_sum.view(b, C, 1)\n",
    "        sigma_sq = sigma_sq.view(b, C, psize)\n",
    "        cost_h = (self.beta_u.view(C, 1) + torch.log(sigma_sq.sqrt())) * r_sum\n",
    "\n",
    "        a_out = self.sigmoid(self._lambda*(self.beta_a - cost_h.sum(dim=2)))\n",
    "        sigma_sq = sigma_sq.view(b, 1, C, psize)\n",
    "\n",
    "        return a_out, mu, sigma_sq\n",
    "\n",
    "    def e_step(self, mu, sigma_sq, a_out, v, eps, b, C):\n",
    "        ln_p_j_h = -1. * (v - mu)**2 / (2 * sigma_sq) - torch.log(sigma_sq.sqrt()) - 0.5*self.ln_2pi\n",
    "\n",
    "        ln_ap = ln_p_j_h.sum(dim=3) + torch.log(a_out.view(b, 1, C))\n",
    "        r = self.softmax(ln_ap)\n",
    "        \n",
    "        return r\n",
    "\n",
    "    def caps_em_routing(self, v, a_in, C, eps):\n",
    "        b, B, c, psize = v.shape\n",
    "        assert c == C\n",
    "        assert (b, B, 1) == a_in.shape\n",
    "\n",
    "        r = torch.cuda.FloatTensor(b, B, C).fill_(1./C)\n",
    "        for iter_ in range(self.iters):\n",
    "            a_out, mu, sigma_sq = self.m_step(a_in, r, v, eps, b, B, C, psize)\n",
    "            if iter_ < self.iters - 1:\n",
    "                r = self.e_step(mu, sigma_sq, a_out, v, eps, b, C)\n",
    "\n",
    "        return mu, a_out\n",
    "\n",
    "    def add_pathes(self, x, B, K, psize, stride):\n",
    "        b, h, w, c = x.shape\n",
    "        assert h == w\n",
    "        assert c == B*(psize+1)\n",
    "        oh = ow = int(((h - K )/stride)+ 1)\n",
    "        idxs = [[(h_idx + k_idx) for k_idx in range(0, K)] for h_idx in range(0, h - K + 1, stride)]\n",
    "        x = x[:, idxs, :, :]\n",
    "        x = x[:, :, :, idxs, :]\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        \n",
    "        return x, oh, ow\n",
    "\n",
    "    def transform_view(self, x, w, C, P, w_shared=False):\n",
    "        b, B, psize = x.shape\n",
    "        assert psize == P*P\n",
    "\n",
    "        x = x.view(b, B, 1, P, P)\n",
    "        if w_shared:\n",
    "            hw = int(B / w.size(1))\n",
    "            w = w.repeat(1, hw, 1, 1, 1)\n",
    "\n",
    "        w = w.repeat(b, 1, 1, 1, 1)\n",
    "        x = x.repeat(1, 1, C, 1, 1)\n",
    "        v = torch.matmul(x, w)\n",
    "        v = v.view(b, B, C, P*P)\n",
    "        return v\n",
    "\n",
    "    def add_coord(self, v, b, h, w, B, C, psize):\n",
    "        assert h == w\n",
    "        v = v.view(b, h, w, B, C, psize)\n",
    "        coor = torch.arange(h, dtype=torch.float32) / h\n",
    "        coor_h = torch.cuda.FloatTensor(1, h, 1, 1, 1, self.psize).fill_(0.)\n",
    "        coor_w = torch.cuda.FloatTensor(1, 1, w, 1, 1, self.psize).fill_(0.)\n",
    "        coor_h[0, :, 0, 0, 0, 0] = coor\n",
    "        coor_w[0, 0, :, 0, 0, 1] = coor\n",
    "        v = v + coor_h + coor_w\n",
    "        v = v.view(b, h*w*B, C, psize)\n",
    "        \n",
    "        return v\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, h, w, c = x.shape\n",
    "        if not self.w_shared:\n",
    "            x, oh, ow = self.add_pathes(x, self.B, self.K, self.psize, self.stride)\n",
    "\n",
    "            p_in = x[:, :, :, :, :, :self.B*self.psize].contiguous()\n",
    "            a_in = x[:, :, :, :, :, self.B*self.psize:].contiguous()\n",
    "            p_in = p_in.view(b*oh*ow, self.K*self.K*self.B, self.psize)\n",
    "            a_in = a_in.view(b*oh*ow, self.K*self.K*self.B, 1)\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P)\n",
    "\n",
    "            p_out, a_out = self.caps_em_routing(v, a_in, self.C, self.eps)\n",
    "            p_out = p_out.view(b, oh, ow, self.C*self.psize)\n",
    "            a_out = a_out.view(b, oh, ow, self.C)\n",
    "            out = torch.cat([p_out, a_out], dim=3)\n",
    "        else:\n",
    "            assert c == self.B*(self.psize+1)\n",
    "            assert 1 == self.K\n",
    "            assert 1 == self.stride\n",
    "            p_in = x[:, :, :, :self.B*self.psize].contiguous()\n",
    "            p_in = p_in.view(b, h*w*self.B, self.psize)\n",
    "            a_in = x[:, :, :, self.B*self.psize:].contiguous()\n",
    "            a_in = a_in.view(b, h*w*self.B, 1)\n",
    "\n",
    "            v = self.transform_view(p_in, self.weights, self.C, self.P, self.w_shared)\n",
    "\n",
    "            if self.coor_add:\n",
    "                v = self.add_coord(v, b, h, w, self.B, self.C, self.psize)\n",
    "\n",
    "            _, out = self.caps_em_routing(v, a_in, self.C, self.eps)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, A, B, C, D, E, K, P, iters_routings):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=A,\n",
    "                               kernel_size=(5,3,3), stride=1, padding=0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=A, out_channels=A,\n",
    "                               kernel_size=(3), stride=2, padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=A, out_channels=A,\n",
    "                               kernel_size=(3), stride=1, padding=0)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=A, out_channels=A,\n",
    "                               kernel_size=(3), stride=1, padding=0)\n",
    "        \n",
    "        self.primary_caps = PrimaryCaps(A, B, 1, P, stride=1)\n",
    "\n",
    "        self.conv_caps1 = ConvCaps(B, C, 5, P, stride=2, iters=iters_routings)\n",
    "\n",
    "        self.conv_caps2 = ConvCaps(C, D, K, P, stride=1, iters=iters_routings)\n",
    "\n",
    "        self.class_caps = ConvCaps(D, E, 1, P, stride=1, iters=iters_routings,\n",
    "                                        coor_add=True, w_shared=True)\n",
    "        \n",
    "        print('=='*20)\n",
    "        print('model info:','\\nA, B, C, D, E, K, P =',str([A,B,C,D,E,K,P])\n",
    "        ,'\\niters for routings: ',str(iters_routings))\n",
    "        print('=='*20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.unsqueeze(1))\n",
    "        x = (F.relu(x)).squeeze()\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.conv3(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.primary_caps(x)\n",
    "        x = self.conv_caps1(x)\n",
    "        x = self.conv_caps2(x)\n",
    "        x = self.class_caps(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ca0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class SpreadLoss(_Loss):\n",
    "\n",
    "    def __init__(self, m_min, m_max, num_class):\n",
    "        super(SpreadLoss, self).__init__()\n",
    "        self.m_min = m_min\n",
    "        self.m_max = m_max\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x, target, r):\n",
    "        b, E = x.shape\n",
    "        assert E == self.num_class\n",
    "        margin = self.m_min + (self.m_max - self.m_min)*r\n",
    "\n",
    "        at = torch.cuda.FloatTensor(b).fill_(0)\n",
    "        for i, lb in enumerate(target):\n",
    "            at[i] = x[i][lb]\n",
    "        at = at.view(b, 1).repeat(1, E)\n",
    "\n",
    "        zeros = x.new_zeros(x.shape)\n",
    "        loss = torch.max(margin - (at - x), zeros)\n",
    "        loss = loss**2\n",
    "        loss = loss.sum() / b - margin**2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950adfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "model = CapsNet(A=64, B=8, C=16, D=16, E=2, K=3, P=2, iters_routings=3).to(device)\n",
    "criterion = SpreadLoss(num_class=2, m_min=0.1, m_max=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(output, target, topk=(1,)):\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n",
    "\n",
    "def exp_lr_decay(optimizer, global_step, init_lr = 1e-4, decay_steps = 20000, decay_rate = 0.96, lr_clip = 1e-4, ext_decay = 1, staircase=False):\n",
    "    if staircase:\n",
    "        lr = (init_lr * decay_rate**(global_step // decay_steps)) \n",
    "    else:\n",
    "        lr = (init_lr * decay_rate**(global_step / decay_steps)) \n",
    "    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    epoch_acc = 0\n",
    "    train_len = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        r = (1.*batch_idx + (epoch-1)*train_len) / (epochs*train_len)\n",
    "        loss = criterion(output, target, r)\n",
    "        acc = Accuracy(output, target)\n",
    "        global_step = (batch_idx+1) + (epoch - 1) * len(train_loader) \n",
    "        exp_lr_decay(optimizer = optimizer, global_step = global_step)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        epoch_acc += acc[0].item()\n",
    "            \n",
    "    loss = train_loss / train_len\n",
    "    accuracy = epoch_acc / train_len     \n",
    "    \n",
    "    print(f'\\n Train Epoch: {epoch} \\tLoss: {loss:.6f}')\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion, device):\n",
    "    \n",
    "    DNN_score = open('/data/github/result/Matrix_CapsNet_DNN_score.TXT', 'a')\n",
    "     \n",
    "    model.eval()\n",
    "    test_len = len(test_loader)\n",
    "    test_loss = 0\n",
    "    acc = 0    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            if batch_idx < int(len(test_data)/batch_size):\n",
    "                for n in range(0,batch_size):\n",
    "                    hh=math.sqrt((output[n][1]**2).sum())\n",
    "                    DNN_score.write(str(hh))\n",
    "                    DNN_score.write('\\n')                   \n",
    "                       \n",
    "            if int(len(test_data)/batch_size) == batch_idx:\n",
    "                for n in range (0,int((len(test_data)/batch_size-int(len(test_data)/batch_size))*batch_size)):\n",
    "                    hh=math.sqrt((output[n][1]**2).sum())\n",
    "                    DNN_score.write(str(hh))\n",
    "                    DNN_score.write('\\n')  \n",
    "                    \n",
    "            test_loss += criterion(output, target, r=1).item()\n",
    "            acc += Accuracy(output, target)[0].item()\n",
    "\n",
    "    loss = test_loss / test_len\n",
    "    accuracy = acc / test_len\n",
    "    \n",
    "    print(f'\\nTest set: Loss: {loss:.6f}, Accuracy: {round(accuracy*((test_len-1)*0.2+0.1))}/{len(test_loader.dataset)}({(accuracy):.2f}%)\\n')\n",
    "    print('----------------------------------------------------')\n",
    "    \n",
    "    DNN_score.close()\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Like(s, b, u, n):\n",
    "    return math.e**((n*s+b)*math.log(u*s+b)-math.lgamma(n*s+b+1)-(u*s+b))\n",
    "def IndiLikeRatioDis(NS1, NB1):\n",
    "    return math.sqrt(-2*math.log((Like(NS1, NB1, 0.0, 1.0))/(Like(NS1, NB1, 1.0, 1.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_score = open('/data/github/result/Matrix_CapsNet_DNN_score.TXT', 'w')    \n",
    "\n",
    "Train_Loss = []\n",
    "Train_Accuracy = []\n",
    "Test_Loss = []\n",
    "Test_Accuracy = []\n",
    "\n",
    "epochs = 1\n",
    "best_sig = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start=time.time()\n",
    "    print('########### Training epoch {} start ###########'.format(epoch))\n",
    "\n",
    "    train_loss, train_accuracy = train(train_loader, model, criterion, optimizer, epoch, device)\n",
    "    test_loss, test_accuracy = test(test_loader, model, criterion, device)\n",
    "\n",
    "    Train_Loss.append(train_loss)\n",
    "    Train_Accuracy.append(train_accuracy)\n",
    "    Test_Loss.append(test_loss)\n",
    "    Test_Accuracy.append(test_accuracy)\n",
    "\n",
    "    Results = np.loadtxt('/data/github/result/Matrix_CapsNet_DNN_score.TXT')\n",
    "\n",
    "    epoch_ = epoch\n",
    "\n",
    "    Results_hh_1=Results[len(test_data)*(epoch_-1):len(test_data)*(epoch_-1)+5000]\n",
    "    Results_tt_1=Results[len(test_data)*(epoch_-1)+5000:len(test_data)*(epoch_-1)+5000+19000]\n",
    "    Results_tw_1=Results[len(test_data)*(epoch_-1)+5000+19000:len(test_data)*(epoch_-1)+5000+19000+800]\n",
    "    Results_tth_1=Results[len(test_data)*(epoch_-1)+5000+19000+800:len(test_data)*(epoch_-1)+5000+19000+800+100]\n",
    "    Results_ttv_1=Results[len(test_data)*(epoch_-1)+5000+19000+800+100:len(test_data)*(epoch_-1)+5000+19000+800+100+100]\n",
    "    Results_llbj_1=Results[len(test_data)*(epoch_-1)+5000+19000+800+100+100:len(test_data)*(epoch_-1)+5000+19000+800+100+100+240]\n",
    "    Results_tatabb_1=Results[len(test_data)*(epoch_-1)+5000+19000+800+100+100+240:len(test_data)*epoch_]\n",
    "\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='Time New Roman')\n",
    "\n",
    "    logs = False\n",
    "\n",
    "    axislabels = [ r'$DNN $']\n",
    "\n",
    "    Yaxislabels = [ r'$DNN $']\n",
    "\n",
    "    Bmax = 1\n",
    "    Bmin = 0\n",
    "    plt.xlim(0, 1)\n",
    "    bins = np.linspace(Bmin, Bmax,  25)\n",
    "    plt.hist(Results_hh_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, color='black', label= r'$h \\; h$')\n",
    "    plt.hist(Results_tt_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, color='blue', label= r'$t \\; \\overline{t}$')\n",
    "    plt.hist(Results_tw_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, color='red', label= r'$t \\; w$')\n",
    "    plt.hist(Results_tth_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, label= r'$t \\; \\overline{t} \\; h$')\n",
    "    plt.hist(Results_ttv_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, label= r'$t \\; \\overline{t} \\; v$')\n",
    "    plt.hist(Results_llbj_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, label= r'$l \\; l \\; b \\; j$')\n",
    "    plt.hist(Results_tatabb_1, bins = bins, alpha=1, density=True, histtype='step', align = 'mid', linewidth = 1.5, log=logs, label= r'$\\tau \\; \\tau \\; b \\; b$')\n",
    "    plt.legend(loc=9,fontsize = 10)\n",
    "    plt.xlabel(axislabels[0], fontsize = 20)\n",
    "    plt.ylabel(r'$\\rm{(1/\\sigma) \\; d \\sigma / d }$' + Yaxislabels[0]    , fontsize = 20)\n",
    "    plt.show()\n",
    "\n",
    "    ROC_Results = open('/data/github/result/Matrix_CapsNet_ROC.TXT'+str(epoch), 'w')\n",
    "\n",
    "    XSig_box = []\n",
    "\n",
    "    Xbkg_box = []\n",
    "\n",
    "    Xbkg_tt_box = []\n",
    "\n",
    "    Xbkg_tw_box = []\n",
    "\n",
    "    Xbkg_tth_box = []\n",
    "\n",
    "    Xbkg_ttv_box = []\n",
    "\n",
    "    Xbkg_llbj_box = []\n",
    "\n",
    "    Xbkg_tatabb_box = []\n",
    "\n",
    "    nn = 10000\n",
    "\n",
    "    Ival = 0.9\n",
    "\n",
    "    Xreco_Sig = 0.0214964\n",
    "\n",
    "    Xreco_tt = 120.907 * 1.596\n",
    "\n",
    "    Xreco_tw = 4.38354\n",
    "\n",
    "    Xreco_tth = 0.15258 * 1.27\n",
    "\n",
    "    Xreco_ttv = 0.157968 * 1.54\n",
    "\n",
    "    Xreco_llbj = 1.22936\n",
    "\n",
    "    Xreco_tatabb = 0.011392\n",
    "\n",
    "    for j in range(0, nn):\n",
    "\n",
    "        roc_sig = 0\n",
    "        roc_bkg_tt = 0\n",
    "        roc_bkg_tw = 0\n",
    "        roc_bkg_tth = 0\n",
    "        roc_bkg_ttv = 0\n",
    "        roc_bkg_llbj = 0\n",
    "        roc_bkg_tatabb = 0\n",
    "\n",
    "        for i in range(0, len(Results_hh_1)):\n",
    "            if Results_hh_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_sig = roc_sig + 1\n",
    "\n",
    "        for i in range(0, len(Results_tt_1 )):\n",
    "            if Results_tt_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_tt = roc_bkg_tt + 1\n",
    "\n",
    "        for i in range(0, len(Results_tw_1) ):\n",
    "            if Results_tw_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_tw = roc_bkg_tw + 1\n",
    "\n",
    "        for i in range(0, len(Results_tth_1 )):\n",
    "            if Results_tth_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_tth = roc_bkg_tth + 1\n",
    "\n",
    "        for i in range(0, len(Results_ttv_1) ):\n",
    "            if Results_ttv_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_ttv = roc_bkg_ttv + 1\n",
    "\n",
    "        for i in range(0, len(Results_llbj_1 )):\n",
    "            if Results_llbj_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_llbj = roc_bkg_llbj + 1\n",
    "\n",
    "        for i in range(0, len(Results_tatabb_1) ):\n",
    "            if Results_tatabb_1[i] > float( Ival + float(0.1*j)/float(nn) ) :\n",
    "                roc_bkg_tatabb = roc_bkg_tatabb + 1\n",
    "\n",
    "        XSig_box.append( float( float(Xreco_Sig)*float( roc_sig ) / float( len(Results_hh_1) ) )   )\n",
    "\n",
    "        Xbkg_box.append( float( float(Xreco_tt)*float( roc_bkg_tt ) / float( len(Results_tt_1) ) ) + float( float(Xreco_tw)*float( roc_bkg_tw ) / float( len(Results_tw_1) ) ) + float( float(Xreco_tth)*float( roc_bkg_tth ) / float( len(Results_tth_1) ) ) + float( float(Xreco_ttv)*float( roc_bkg_ttv ) / float( len(Results_ttv_1) ) ) + float( float(Xreco_llbj)*float( roc_bkg_llbj ) / float( len(Results_llbj_1) ) ) + float( float(Xreco_tatabb)*float( roc_bkg_tatabb ) / float( len(Results_tatabb_1) ) )   )\n",
    "\n",
    "        Xbkg_tt_box.append( float( float(Xreco_tt)*float( roc_bkg_tt ) / float( len(Results_tt_1) ) )  )\n",
    "\n",
    "        Xbkg_tw_box.append( float( float(Xreco_tw)*float( roc_bkg_tw ) / float( len(Results_tw_1) ) )   )\n",
    "\n",
    "        Xbkg_tth_box.append( float( float(Xreco_tth)*float( roc_bkg_tth ) / float( len(Results_tth_1) ) )  )\n",
    "\n",
    "        Xbkg_ttv_box.append( float( float(Xreco_ttv)*float( roc_bkg_ttv ) / float( len(Results_ttv_1) ) )   )\n",
    "\n",
    "        Xbkg_llbj_box.append( float( float(Xreco_llbj)*float( roc_bkg_llbj ) / float( len(Results_llbj_1) ) )  )\n",
    "\n",
    "        Xbkg_tatabb_box.append( float( float(Xreco_tatabb)*float( roc_bkg_tatabb ) / float( len(Results_tatabb_1) ) )   )    \n",
    "\n",
    "    for j in range(0, len(XSig_box) ):\n",
    "\n",
    "        if float( Xbkg_box[j] ) == 0 :\n",
    "            break\n",
    "\n",
    "        Nsig = round( float( 3000*XSig_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg = round( float( 3000*Xbkg_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_tt = round( float( 3000*Xbkg_tt_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_tw = round( float( 3000*Xbkg_tw_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_tth = round( float( 3000*Xbkg_tth_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_ttv = round( float( 3000*Xbkg_ttv_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_llbj = round( float( 3000*Xbkg_llbj_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        Nbkg_tatabb = round( float( 3000*Xbkg_tatabb_box[j]*(0.8**2/0.7**2) ), 3)\n",
    "\n",
    "        SobSqrtB = round( float( IndiLikeRatioDis(float( Nsig ),float( Nbkg ) )  ) ,   3 )\n",
    "        ROC_Results.write(str(Nsig) + ' ' + str(Nbkg) + ' ' + str(SobSqrtB) + ' ' + str(Nbkg_tt) + ' ' + str(Nbkg_tw) + ' ' + str(Nbkg_tth) + ' ' + str(Nbkg_ttv) + ' ' + str(Nbkg_llbj) + ' ' + str(Nbkg_tatabb)   ) \n",
    "        ROC_Results.write('\\n')\n",
    "\n",
    "    ROC_Results.close()\n",
    "\n",
    "    ROC_Results= np.loadtxt('/data/github/result/Matrix_CapsNet_ROC.TXT'+str(epoch))\n",
    "\n",
    "    SB=[]\n",
    "    hh=[]\n",
    "    tt=[]\n",
    "    tw=[]\n",
    "    tth=[]\n",
    "    ttv=[]\n",
    "    llbj=[]\n",
    "    tatabb=[]\n",
    "\n",
    "    for n in range(len(ROC_Results)):  \n",
    "        SB.append(ROC_Results[n][2])\n",
    "        hh.append(ROC_Results[n][0])\n",
    "        tt.append(ROC_Results[n][3])\n",
    "        tw.append(ROC_Results[n][4])\n",
    "        tth.append(ROC_Results[n][5])\n",
    "        ttv.append(ROC_Results[n][6])\n",
    "        llbj.append(ROC_Results[n][7])\n",
    "        tatabb.append(ROC_Results[n][8])\n",
    "\n",
    "    plt.plot(hh,SB, color='r', label='Significance')\n",
    "    plt.xlabel(r'$ N_s $', fontsize=20)\n",
    "    plt.ylabel(r'Significance', fontsize=20)\n",
    "    plt.legend(loc='best', fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "    j=SB.index(max(SB))\n",
    "    print('\\nsignificance: {:.3f} hh: {:.3f} tt: {:.3f} tw: {:.3f} tth: {:.3f} ttv: {:.3f} llbj: {:.3f} tatabb: {:.3f} \\n'.format(SB[j], hh[j], tt[j], tw[j], tth[j], ttv[j], llbj[j], tatabb[j]))         \n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        sig = max(SB)\n",
    "        best_sig = max(best_sig,sig)           \n",
    "\n",
    "    end=time.time()\n",
    "\n",
    "    print('* Best Significance : {:.3f} *'.format(best_sig))\n",
    "\n",
    "    print('Epoch time: {:.2f} mins'.format((end-start)/60))\n",
    "    print('='*69)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun",
   "language": "python",
   "name": "jun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
